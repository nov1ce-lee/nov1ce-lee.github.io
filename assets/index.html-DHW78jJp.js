import{_ as n,c as d,a as s,d as t,e,b as r,w as l,r as h,o}from"./app-CJtV0nGW.js";const p={};function c(k,a){const i=h("VPLink");return o(),d("div",null,[a[6]||(a[6]=s(`<h2 id="官方介绍" tabindex="-1"><a class="header-anchor" href="#官方介绍"><span>官方介绍</span></a></h2><p><img src="https://obsidian-pic-1326566629.cos.ap-shanghai.myqcloud.com/20250728144422853.png" alt="" loading="lazy"><a href="https://llamafactory.readthedocs.io/zh-cn/latest/index.html" target="_blank" rel="noopener noreferrer">官方文档</a><br><a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener noreferrer">GitHub</a></p><h2 id="如何使用" tabindex="-1"><a class="header-anchor" href="#如何使用"><span>如何使用</span></a></h2><h3 id="安装" tabindex="-1"><a class="header-anchor" href="#安装"><span>安装</span></a></h3><div class="hint-container important"><p class="hint-container-title">重要</p><p>此步骤为必需，建议使用虚拟环境安装避免不同项目包冲突</p></div><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">git</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> clone</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> --depth</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 1</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> https://github.com/hiyouga/LLaMA-Factory.git</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">cd</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> LLaMA-Factory</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">pip</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> install</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -e</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">.[torch,metrics]</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="数据准备" tabindex="-1"><a class="header-anchor" href="#数据准备"><span>数据准备</span></a></h3><p>数据集文件的格式：<a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md" target="_blank" rel="noopener noreferrer">规范文档</a><br> 你可以使用 HuggingFace / ModelScope / Modelers 上的数据集或加载本地数据集</p><div class="hint-container note"><p class="hint-container-title">注</p><p>当你使用自定义数据集时，请更新<code>data/dataset_info.json</code>文件</p></div><h3 id="快速开始" tabindex="-1"><a class="header-anchor" href="#快速开始"><span>快速开始</span></a></h3><p>下面三行命令分别对<code>Llama3-8B-Instruct</code>模型进行<code>LoRA</code>微调、推理和合并</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">llamafactory-cli</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> train</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> examples/train_lora/llama3_lora_sft.yaml</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">llamafactory-cli</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> chat</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> examples/inference/llama3_lora_sft.yaml</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">llamafactory-cli</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> export</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> examples/merge_lora/llama3_lora_sft.yaml</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="可视化微调" tabindex="-1"><a class="header-anchor" href="#可视化微调"><span>可视化微调</span></a></h3><p><code>LLaMA Board</code>可视化微调(由<a href="https://github.com/gradio-app/gradio" target="_blank" rel="noopener noreferrer">Gradio</a>驱动):</p><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">llamafactory-cli</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> webui</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="高级用法-包括多-gpu-微调" tabindex="-1"><a class="header-anchor" href="#高级用法-包括多-gpu-微调"><span>高级用法（包括多 GPU 微调）</span></a></h3>`,16)),t("p",null,[a[1]||(a[1]=e("请参考",-1)),r(i,{href:"%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95.md"},{default:l(()=>a[0]||(a[0]=[e("高级用法",-1)])),_:1,__:[0]}),a[2]||(a[2]=e("(本页面)",-1)),a[3]||(a[3]=t("br",null,null,-1)),a[4]||(a[4]=e(" & ",-1)),a[5]||(a[5]=t("a",{href:"https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md",target:"_blank",rel:"noopener noreferrer"},"大模型微调示例脚本",-1))]),a[7]||(a[7]=s('<h2 id="模型下载" tabindex="-1"><a class="header-anchor" href="#模型下载"><span>模型下载</span></a></h2><h3 id="从魔搭社区下载" tabindex="-1"><a class="header-anchor" href="#从魔搭社区下载"><span>从魔搭社区下载</span></a></h3><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">export</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> USE_MODELSCOPE_HUB</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # Windows 使用 `set USE_MODELSCOPE_HUB=1`</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>将 <code>model_name_or_path</code> 设置为模型 ID 来加载对应的模型。在<a href="https://modelscope.cn/models" target="_blank" rel="noopener noreferrer">魔搭社区</a>查看所有可用的模型，例如 <code>LLM-Research/Meta-Llama-3-8B-Instruct</code></p><h3 id="从魔乐社区下载" tabindex="-1"><a class="header-anchor" href="#从魔乐社区下载"><span>从魔乐社区下载</span></a></h3><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">export</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> USE_OPENMIND_HUB</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1</span><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"> # Windows 使用 `set USE_OPENMIND_HUB=1`</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>将 <code>model_name_or_path</code> 设置为模型 ID 来加载对应的模型。在<a href="https://modelers.cn/models" target="_blank" rel="noopener noreferrer">魔乐社区</a>查看所有可用的模型，例如 <code>TeleAI/TeleChat-7B-pt</code></p><h2 id="硬件依赖估算值" tabindex="-1"><a class="header-anchor" href="#硬件依赖估算值"><span>硬件依赖估算值</span></a></h2><table><thead><tr><th>方法</th><th>精度</th><th>7B</th><th>14B</th><th>30B</th><th>70B</th><th>xB</th></tr></thead><tbody><tr><td>Full (bf16 or fp16)</td><td>32</td><td>120GB</td><td>240GB</td><td>600GB</td><td>1200GB</td><td>18xGB</td></tr><tr><td>Full (pure_bf16)</td><td>16</td><td>60GB</td><td>120GB</td><td>300GB</td><td>600GB</td><td>8xGB</td></tr><tr><td>Freeze/LoRA/GaLore/APOLLO/BAdam</td><td>16</td><td>16GB</td><td>32GB</td><td>64GB</td><td>160GB</td><td>2xGB</td></tr><tr><td>QLoRA</td><td>8</td><td>10GB</td><td>20GB</td><td>40GB</td><td>80GB</td><td>xGB</td></tr><tr><td>QLoRA</td><td>4</td><td>6GB</td><td>12GB</td><td>24GB</td><td>48GB</td><td>x/2GB</td></tr><tr><td>QLoRA</td><td>2</td><td>4GB</td><td>8GB</td><td>16GB</td><td>24GB</td><td>x/4GB</td></tr></tbody></table>',9))])}const m=n(p,[["render",c]]),b=JSON.parse('{"path":"/notes/aillm/fine-tuning/llama-factory/","title":"使用手册","lang":"zh-CN","frontmatter":{"title":"使用手册","createTime":"2025/03/16 13:24:26","permalink":"/notes/aillm/fine-tuning/llama-factory/","description":"官方介绍 官方文档 GitHub 如何使用 安装 重要 此步骤为必需，建议使用虚拟环境安装避免不同项目包冲突 数据准备 数据集文件的格式：规范文档 你可以使用 HuggingFace / ModelScope / Modelers 上的数据集或加载本地数据集 注 当你使用自定义数据集时，请更新data/dataset_info.json文件 快速开始 ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"使用手册\\",\\"image\\":[\\"https://obsidian-pic-1326566629.cos.ap-shanghai.myqcloud.com/20250728144422853.png\\"],\\"dateModified\\":\\"2025-12-11T16:51:13.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://nov1ce-lee.github.io/notes/aillm/fine-tuning/llama-factory/"}],["meta",{"property":"og:site_name","content":"novice.log"}],["meta",{"property":"og:title","content":"使用手册"}],["meta",{"property":"og:description","content":"官方介绍 官方文档 GitHub 如何使用 安装 重要 此步骤为必需，建议使用虚拟环境安装避免不同项目包冲突 数据准备 数据集文件的格式：规范文档 你可以使用 HuggingFace / ModelScope / Modelers 上的数据集或加载本地数据集 注 当你使用自定义数据集时，请更新data/dataset_info.json文件 快速开始 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://obsidian-pic-1326566629.cos.ap-shanghai.myqcloud.com/20250728144422853.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-11T16:51:13.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-11T16:51:13.000Z"}]]},"readingTime":{"minutes":1.4,"words":421},"git":{"createdTime":1765471873000,"updatedTime":1765471873000,"contributors":[{"name":"nov1ce","username":"nov1ce","email":"289836737@qq.com","commits":1,"avatar":"https://avatars.githubusercontent.com/nov1ce?v=4","url":"https://github.com/nov1ce"}]},"autoDesc":true,"filePathRelative":"notes/aillm/微调训练/LLaMA Factory/1.使用手册.md","headers":[]}');export{m as comp,b as data};
