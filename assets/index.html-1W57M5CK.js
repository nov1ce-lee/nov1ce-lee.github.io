import{_ as h,c as k,a as t,b as d,w as a,r as p,o as r,d as i}from"./app-DqdawiVA.js";const g={};function c(o,s){const n=p("CodeTabs");return r(),k("div",null,[s[6]||(s[6]=t(`<blockquote><p>教程整理自csdn文章 <a href="https://blog.csdn.net/mingzai624/article/details/140881097" target="_blank" rel="noopener noreferrer">将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行 —— 以 Qwen2-0.5B 为例</a> 和 github的Discussions <a href="https://github.com/ggml-org/llama.cpp/discussions/2948" target="_blank" rel="noopener noreferrer">教程：如何将 HuggingFace 模型转换为 GGUF 格式</a></p></blockquote><h2 id="目的和介绍" tabindex="-1"><a class="header-anchor" href="#目的和介绍"><span>目的和介绍</span></a></h2><p>借助<code>llama.cpp</code>将<code>safetensors</code>格式转<code>gguf</code></p><h3 id="什么是gguf" tabindex="-1"><a class="header-anchor" href="#什么是gguf"><span>什么是GGUF</span></a></h3><p><code>GGUF</code> 全称为 <code>GPT-Generated Unified Format</code> ，是一种专为大语言模型设计的二进制文件格式，旨在实现模型的快速加载和保存，同时易于读取。</p><h3 id="为什么要转换为gguf格式" tabindex="-1"><a class="header-anchor" href="#为什么要转换为gguf格式"><span>为什么要转换为GGUF格式</span></a></h3><p>GGUF具有很多优势，包括：</p><ul><li>单文件部署：模型可以轻松分发和加载，不需要任何外部文件来提供额外信息</li><li>可拓展性：可以在不破坏与现有模型的兼容性的情况下，向基于GGML的执行器添加新功能或向GGUF模型添加新信息。</li><li>mmap兼容性：可以使用mmap加载模型，以实现快速加载和保存。</li><li>易于使用：可以使用少量代码轻松加载和保存模型，无论使用何种语言，无需外部库。</li><li>完整信息：加载模型所需的所有信息都包含在模型文件中，用户无需提供任何额外信息。</li></ul><h2 id="模型转换步骤" tabindex="-1"><a class="header-anchor" href="#模型转换步骤"><span>模型转换步骤</span></a></h2><h3 id="配置转换环境" tabindex="-1"><a class="header-anchor" href="#配置转换环境"><span>配置转换环境</span></a></h3><ul><li>克隆llama.cpp仓库</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">git</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> clone</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> https://github.com/ggerganov/llama.cpp.git</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>建议使用python虚拟环境，创建新的环境并安装所需依赖库</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># conda环境配置</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">conda</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> create</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> --name</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> llama.cpp</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> python=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">3.10</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">conda</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> activate</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> llama.cpp</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">pip</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> install</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -r</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> llama.cpp/requirements.txt</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h3 id="执行格式转换" tabindex="-1"><a class="header-anchor" href="#执行格式转换"><span>执行格式转换</span></a></h3><ul><li>使用<code>llama.cpp</code>仓库的<code>convert_hf_to_gguf.py</code>脚本转换</li></ul>`,17)),d(n,{id:"83",data:[{id:"格式"},{id:"全参数转换（无精度损失）"},{id:"量化转换（推理加速但有精度损失）"}]},{title0:a(({value:e,isActive:l})=>s[0]||(s[0]=[i("span",null,"格式",-1)])),title1:a(({value:e,isActive:l})=>s[1]||(s[1]=[i("span",null,"全参数转换（无精度损失）",-1)])),title2:a(({value:e,isActive:l})=>s[2]||(s[2]=[i("span",null,"量化转换（推理加速但有精度损失）",-1)])),tab0:a(({value:e,isActive:l})=>s[3]||(s[3]=[i("div",{class:"language-bash line-numbers-mode","data-highlighter":"shiki","data-ext":"bash",style:{"--shiki-light":"#393a34","--shiki-dark":"#dbd7caee","--shiki-light-bg":"#ffffff","--shiki-dark-bg":"#121212"}},[i("pre",{class:"shiki shiki-themes vitesse-light vitesse-dark vp-code"},[i("code",{class:"language-bash"},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#59873A","--shiki-dark":"#80A665"}},"python"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," llama.cpp/convert_hf_to_gguf.py"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," {路径}"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}}," --outtype"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," {type}"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}}," --outfile"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," {filepath/filename}")])])]),i("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[i("div",{class:"line-number"})])],-1)])),tab1:a(({value:e,isActive:l})=>s[4]||(s[4]=[i("div",{class:"language-bash line-numbers-mode","data-highlighter":"shiki","data-ext":"bash",style:{"--shiki-light":"#393a34","--shiki-dark":"#dbd7caee","--shiki-light-bg":"#ffffff","--shiki-dark-bg":"#121212"}},[i("pre",{class:"shiki shiki-themes vitesse-light vitesse-dark vp-code"},[i("code",{class:"language-bash"},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#59873A","--shiki-dark":"#80A665"}},"python"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," llama.cpp/convert_hf_to_gguf.py"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," ./models/qwen2.5-1.5b-instruct-merged"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}},"  --outtype"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," f16"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}}," --verbose"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}}," --outfile"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," ./models/qwen2.5-1.5b-instruct_f16.gguf")])])]),i("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[i("div",{class:"line-number"})])],-1)])),tab2:a(({value:e,isActive:l})=>s[5]||(s[5]=[i("div",{class:"language-bash line-numbers-mode","data-highlighter":"shiki","data-ext":"bash",style:{"--shiki-light":"#393a34","--shiki-dark":"#dbd7caee","--shiki-light-bg":"#ffffff","--shiki-dark-bg":"#121212"}},[i("pre",{class:"shiki shiki-themes vitesse-light vitesse-dark vp-code"},[i("code",{class:"language-bash"},[i("span",{class:"line"},[i("span",{style:{"--shiki-light":"#59873A","--shiki-dark":"#80A665"}},"python"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," llama.cpp/convert_hf_to_gguf.py"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," ./models/qwen2.5-1.5b-instruct-merged"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}},"  --outtype"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," q8_0"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}}," --verbose"),i("span",{style:{"--shiki-light":"#A65E2B","--shiki-dark":"#C99076"}}," --outfile"),i("span",{style:{"--shiki-light":"#B56959","--shiki-dark":"#C98A7D"}}," ./models/qwen2.5-1.5b-instruct_q8_0.gguf")])])]),i("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[i("div",{class:"line-number"})])],-1)])),_:1}),s[7]||(s[7]=t(`<div class="hint-container note"><p class="hint-container-title">注</p><p><code>--outtype</code> 是输出类型，代表含义：</p><table><thead><tr><th>outtype</th><th>meaning</th></tr></thead><tbody><tr><td>q2_k</td><td>特定张量（Tensor）采用较高的精度设置，其他的则保持基础级别</td></tr><tr><td>q3_k_l, q3_k_m, q3_k_s</td><td>在不同张量上使用不同级别的精度，从而达到性能和效率的平衡</td></tr><tr><td>q4_0</td><td>最初的量化方案，使用 4 位精度</td></tr><tr><td>q4_1, q4_k_m, q4_k_s</td><td>提供不同程度的准确性和推理速度，适合需要平衡资源使用的场景</td></tr><tr><td>q5_0, q5_1, q5_k_m, q5_k_s</td><td>保证更高准确度的同时，使用更多的资源且推理速度较慢</td></tr><tr><td>q6_k, q8_0</td><td>提供最高的精度，但性能需求大时间消耗多</td></tr><tr><td>fp16, f32</td><td>不量化，保留原始精度</td></tr></tbody></table></div><h2 id="运行gguf格式模型" tabindex="-1"><a class="header-anchor" href="#运行gguf格式模型"><span>运行GGUF格式模型</span></a></h2><h3 id="ollama" tabindex="-1"><a class="header-anchor" href="#ollama"><span>ollama</span></a></h3><p>高度简化AI模型的本地部署与运行</p><ul><li>安装ollama</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">curl</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -fsSL</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> https://ollama.com/install.sh</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> |</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>启动ollama</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">ollama</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> serve</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>创建模型参数文件 创建名为&quot;ModelFile&quot;的meta文件，写入内容：</li></ul><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-py"><span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">FROM</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;"> /</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">mnt</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">/</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">workspace</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">/</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">qwen2</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">5b</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">instruct</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">-</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">q8_0</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">gguf</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># set the temperature to 0.7 [higher is more creative, lower is more coherent]</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">PARAMETER</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> temperature </span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.7</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">PARAMETER</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> top_p </span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">0.8</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">PARAMETER</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> repeat_penalty </span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">1.05</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">TEMPLATE</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;&quot;&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{{ if .System }}&lt;|im_start|&gt;system</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{{ .System }}&lt;|im_end|&gt;</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{{ .Prompt }}&lt;|im_end|&gt;</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{{ end }}&lt;|im_start|&gt;assistant</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">{{ .Response }}&lt;|im_end|&gt;</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#A0ADA0;--shiki-dark:#758575DD;"># set the system message</span></span>
<span class="line"><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;">SYSTEM</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">You are a helpful assistant.</span></span>
<span class="line"><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;&quot;&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>创建自定义模型</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">ollama</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> create</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> qwen2_0.5b_instruct</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> --file</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> ./ModelFile</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>运行模型</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">ollama</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> run</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> qwen2_0.5b_instruct</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="llama-cpp" tabindex="-1"><a class="header-anchor" href="#llama-cpp"><span>llama.cpp</span></a></h3><h3 id="pocketpal" tabindex="-1"><a class="header-anchor" href="#pocketpal"><span>PocketPal</span></a></h3><h2 id="gguf模型推送至huggingface" tabindex="-1"><a class="header-anchor" href="#gguf模型推送至huggingface"><span>GGUF模型推送至HuggingFace</span></a></h2><ul><li>创建名为 <code>upload.py</code> 的Python脚本，写入如下内容</li></ul><div class="language-py line-numbers-mode" data-highlighter="shiki" data-ext="py" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-py"><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> huggingface_hub </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> HfApi</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">api </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> HfApi</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">model_id </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;"> &quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">Noviceey/qwen2.5-1.5b-instruct_q8_0</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">api</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">create_repo</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">model_id</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> exist_ok</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">True</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> repo_type</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">model</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">api</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">upload_file</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">    path_or_fileobj</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">models/qwen2.5-1.5b-instruct_q8_0.gguf</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">    path_in_repo</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;">qwen2.5-1.5b-instruct_q8_0.gguf</span><span style="--shiki-light:#B5695977;--shiki-dark:#C98A7D77;">&quot;</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">    repo_id</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">model_id</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span></span>
<span class="line"><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>从<a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">这里</a>获取具有写入权限的HuggingFace Token</li><li>设置Huggingface token</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">export</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;"> HUGGING_FACE_HUB_TOKEN</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&lt;</span><span style="--shiki-light:#B07D48;--shiki-dark:#BD976A;">paste-your-own-token</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>运行 <code>upload.py</code> 脚本</li></ul><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">python</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> upload.py</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="编译llama-cpp" tabindex="-1"><a class="header-anchor" href="#编译llama-cpp"><span>编译llama.cpp</span></a></h2><ul><li>进入llama.cpp文件夹，进行编译</li></ul><div class="hint-container note"><p class="hint-container-title">注</p><p>目前官网更推荐使用cmake对程序进行编译<br> For more details, see <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md" target="_blank" rel="noopener noreferrer">build.md</a></p></div><div class="language-bash line-numbers-mode" data-highlighter="shiki" data-ext="bash" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-bash"><span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">cd</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> llama.cpp</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">cmake</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> -B</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> build</span></span>
<span class="line"><span style="--shiki-light:#59873A;--shiki-dark:#80A665;">cmake</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> --build</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> build</span><span style="--shiki-light:#A65E2B;--shiki-dark:#C99076;"> --config</span><span style="--shiki-light:#B56959;--shiki-dark:#C98A7D;"> Release</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,27))])}const m=h(g,[["render",c]]),y=JSON.parse('{"path":"/notes/aillm/fine-tuning/lammacpp/","title":"模型格式转换","lang":"zh-CN","frontmatter":{"title":"模型格式转换","createTime":"2025/04/03 19:53:43","permalink":"/notes/aillm/fine-tuning/lammacpp/","description":"教程整理自csdn文章 将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行 —— 以 Qwen2-0.5B 为例 和 github的Discussions 教程：如何将 HuggingFace 模型转换为 GGUF 格式 目的和介绍 借助llama.cpp将safetensors格式转gguf 什么是GGUF GGUF 全称为...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"模型格式转换\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-11T16:51:13.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://novishare.site/notes/aillm/fine-tuning/lammacpp/"}],["meta",{"property":"og:site_name","content":"Novishare.Site"}],["meta",{"property":"og:title","content":"模型格式转换"}],["meta",{"property":"og:description","content":"教程整理自csdn文章 将 HuggingFace 模型转换为 GGUF 及使用 ollama 运行 —— 以 Qwen2-0.5B 为例 和 github的Discussions 教程：如何将 HuggingFace 模型转换为 GGUF 格式 目的和介绍 借助llama.cpp将safetensors格式转gguf 什么是GGUF GGUF 全称为..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-11T16:51:13.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-11T16:51:13.000Z"}]]},"readingTime":{"minutes":3.06,"words":917},"git":{"createdTime":1765471873000,"updatedTime":1765471873000,"contributors":[{"name":"nov1ce","username":"nov1ce","email":"289836737@qq.com","commits":1,"avatar":"https://avatars.githubusercontent.com/nov1ce?v=4","url":"https://github.com/nov1ce"}]},"autoDesc":true,"filePathRelative":"notes/aillm/微调训练/LLaMA Factory/4.llama.cpp.md","headers":[]}');export{m as comp,y as data};
